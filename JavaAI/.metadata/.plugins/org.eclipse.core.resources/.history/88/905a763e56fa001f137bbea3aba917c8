package ann;

//represents the activation of a previous layer
//only works on vectors
public class Activation extends Layer {
	
	ActivationFunction func;
	
	//needed for backprop
	Tensor activation;
	
	//no shape (its the same as the previous layer's)
	public Activation(Layer prev, ActivationFunction func) {
		super(prev, null);
		this.func = func;
		
		activation = null;
	}
	
	@Override
	public Shape getShape() {
		return super.getPrev().getShape();
	}
	
	@Override
	public int numTrainableParams() {
		return 0;
	}
	
	@Override
	public Tensor[] getWeights() {
		return null;
	}
	
	@Override
	public Tensor forward(Tensor in) {
		//store activation for this layer
		activation = new Tensor(in);
		
		switch(func) {
		case RELU:
			//yay fall through on purpose (so easy man)
		case SIGMOID:
			//element wise activation
			for(int i = 0; i< in.data.length; i++) in.data[i] = func.activate(in.data[i]);
			break;
		case SOFTMAX:
			//last dimension gets softmaxxed
			//represents the length of each rank 1 tensor (what we are softmaxxing on)
			int len = in.shape.getDim(0);
			
			//represents the number of softmaxxes to do (or how many rank 1 tensors are in the tensor)
			int numMaxes = in.data.length / len;
			for(int softmax = 0; softmax< numMaxes; softmax++) {
				//calc offset from start
				int start = softmax * len;
				
				//find max element
				float max = Float.NEGATIVE_INFINITY;
				for(int i = start; i < start+len; i++) {
					if (in.data[i] > max) {
		                max = in.data[i];
		            }
				}
				
				//sum the exp'd versions of the elements in the vector
				float sumExp = 0.0f;
				for(int i = start; i < start+len; i++) {
					sumExp += Math.exp(in.data[i] - max);
				}
				
				//now exp each element and normalize it by the sum
				for(int i = start; i< start+len; i++) {
					in.data[i] = (float) (Math.exp(in.data[i] - max)) / sumExp;
				}
			}
			break;
		default:
			break;
		}
		
		return in;
	}

	@Override
	public Tensor backprop(Tensor in) {
		switch(func) {
		case RELU:
			//fall through on purpose
		case SIGMOID:
			for(int i = 0; i< activation.data.length; i++) activation.data[i] = func.activateddx(activation.data[i]);
			break;
		default:
			break;
		}
		
		//broadcast activations for mul
		activation.shape.batch();
		in.mul(activation);
		
		//return
		return in;
	}
}
